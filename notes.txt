Container
- Kubernetes requires app to be packaged into container image.
- Docker supports having multiple versions under same image.
- Docker file is instructions that Docker will perform when building the image.
- Images is composed of multiple layered.

    docker run busybox echo "Hello world"
    - run the image

GKE
- Use gcloud tool to configure default settings.
- https://cloud.google.com/kubernetes-engine/docs/quickstart

Run app on kubernetes
- prepare JSON or YAML manifest, containing a description of all the resources you want to deploy.

Components 
    Nodes
    - Each node runs "Docker", "Kubelet" and "kube-proxy".
    Master Node
    - controls while kube system (control plane).
    Kubectl
    - client command tool which issues REST requests to "Kube API Server"
    
    
Flow of deploying resources.
    1. developer create & push image to "Docker Hub"
    2. developer define & create resource using "kubectl"
    3. kubectl issues REST call to "Kube API Server"
    4. Resource is schedule to a worker node by "Scheduler"
    5. "Kubelet" on the node is notified.
    6. The Node pull the image and run the container from "Docker Hub"

Resource components
- "metadata" includes the name, labels and annotations about the resource.
- "spec" contains the actual description. (eg. contents, env, volumes or other config data)
- "status" contains the current info about the running resource.

Reference
- "kube explain [resourceType]" show you reference documentation for the given resource.
- https://kubernetes.io/docs/reference/kubernetes-api/

Namespace
- organize resources into groups.
- to isolate groups from each other.
    
apiVersion: v1
kind: Namespace
metadata:
  name: [nsName]

Pods
- basic building block in Kube.
- co-located group of containers that will runs on the same node.
- "logical machine" with its own IP, hostname, processes.
- pods spread out across different worker nodes.
- A pod never spans two nodes.

    Splitting into multiple pods.
    - Scaling for each demand.
    - Distribution for tacking advantage of the cluster's resource
    
    Deciding when to use multiple containers in a pod.
    - can they on different hosts?
    - Are then independent components?
    - Must test be scaled together or individually?
    
    Creating pods from yaml.
    - posting YAML manifest to the "Kube API Server"
    https://kubernetes.io/docs/reference/
    
    Container logs
    - kube logs [podName] -c [ctName]
    - automatically rotated daily and every time the log file reaches 10MB.
    
    Port Forwarding
    - no dedicated server, but can connecting to a pod for "debugging".
    - forwarding "local port" to "a port" in the pod.
    - "kubectl port-forward"
    
    Labels (metadata.labels)
        - Organizing Pods
        - key-value pair attached to a resource.
        - selecting resources using label selectors.
        - eg. "app" - kind of service, "rel" - release version
        - Label selectors allow you to select a subset of pods. "kube get -l" 
            - contains a label with a certain key
            - contains a label with a certain key and value
            - contains a label with a certain key, but with value not equal to the one you specify
        
        NodeSelector (spec.nodeSelector)
        - tells kube to deploy this pod only to nodes containing same label.
    
    Annotations (metadata.annotations)
    - hold much larger information than labels.
    
    apiVersion: v1
    kind: Pod
    metadata:
      name: [podName]
      labels:
        [key]: [value]
        ..
    spec:
      nodeselector:
        [key]: [value]
      containers:
        - name: [ctName]
          image: [imgName]
          env:
            - name: [envName]
              value: [envVal]
          ports:
            - name
              containerPort: [ctPort]
              protocol: TCP
              
    Liveness probes
    - check container's health through liveness probes.
    - specify "liveness probe" in the "pod's specification".
    - Kube will periodically execute and if a "probe fail", it will "restart" the container.
        
        Components:
        HTTP GET probe
        - performs an HTTP GET request on the container's IP address, a port and path you specify.
        - "delay", "timeout", "period"
        - already remember to set an initial delay to account for app's startup time
        - should always define a liveness probe.
        
        TCP Socket probe
        - open a TCP connection to the specified port of the container. (check whether connection is established or not)
        
        Exec probe
        - executes an arbitrary command in the container.
        
    apiVersion: v1
    kind: Pod
    metadata:
        ...
    spec:
      containers:
        - name: kubia
        ...
          livenessProbe:
            failureThreshold: 1
            initialDelaySeconds: 15
            timeoutSeconds: 5
            periodSeconds: 15
            httpGet:
              port: 8080
              path: /
                  
    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=1
    
Job
- for a single completable task.
- in the event of a failure of the process, the Job can be configured to either restart the container or not.

apiVersion: batch/v1
kind: Job
metadata:
    ...
spec:
  template:
    metadata:
      labels:
        [key]: [value]
    spec:
      completions: [numOfTasks]
      parallelism: [numberOfThreads]
      restartPolicy: OnFailure
      containers:
        ...

ReplicationController
- used to replicate pods. (high-availability)

Service
- distribute the requests across multiple pods.
- a single constant IP and port for pods (pod is ephemeral and pod IP addresses are ever-changing)

ReplicationControllers
- a resource that ensures its pods are always kept running.
- create and manage multiple copies (replicas) of a pod.
- If pod disappears, rs notices the missing pod and creates a replacement pod. 
- makes sure the actual number of pods of a "a certain label" always matches the desired number (replicas)
- "pod template" can be modified at any time and will have effect on newly created pod.
- horizontally scale up by "kubectl scale rc [resourceName] --replicas=[replicas]"
- when you delete a rc through kubectl delete, the pods are also deleted. 
    Components:
    A Label Selector
    - determines what pods are in rc's scope
    A Replica Count
    - desired number of pods
    A Pod Template
    - is used when rc creates new pod replicas.

    Changing label for a pod.
    - if a pod is malfunctioning, take the pod out of the rc scope (by changing the label - rc will bring new one), 
    and then debug with the pod.
apiVersion: v1
kind: ReplicationController
metadata:
  ...
  labels:
    [key]: [value]
spec:
  replicas: [replicas]
  selector: #pod selector determining what pods the rc cares
    [key]: [value]
  template: #the pod template for creating new pods.
    metadata:
      ...
      labels:
        [key]: value
    spec:
      containers:
      ...

    ReplicaSet
    - new generation of rc.
    - has more expressive pod selectors.
    
    ReplicaSet Selector
        matchLabels
        - listing labels the pods need to have
        matchExpressions
        - match labels by expression "In", "NotIn", "Exists", "DoesNotExist"
        
    DemonSets
    - want a pod to run on each and every node in the cluster
    - useful for infrastructure-related pods that perform system-level operations.
    eg. log collector or resource monitor on every node or kube-proxy process.

Service
- respond to HTTP req coming either from pods inside the cluster of from clients outside the cluster.
- a single, constant point  of entry to a group of pods.
- each has an constant IP address and port.
- label selectors define which pods are part of the service.

    Problems
    - pods are ephemeral
    - IP is assigned after the pod has been scheduled to a node.
    - Pods are replicated & distributed.
   
   Components
   - CLUSTER-IP is for access from inside the cluster.
   - Session Affinity property allow client to be redirected to the same pod everytime.
   - port refers to "svc port"
   - targetPort refers to "pod's port"
   - nodePort refers to "node's port"
   - Discovering services via ENV or DNS
   
   EndPoints
   - EndPoints represents pods' ips and ports.
   - It's kube resource. [kube get endpoints [podName]]
   - Manually created endpoints can be used for external service. (the service can be used like any regual service)
   
   apiVersion: v1
   kind: Service
   metadata:
     name: ext-svc  //should be same
     namespace: dev
   spec:
     ports:
     - port: 80
   ---
   apiVersion: v1
   kind: Endpoints
   metadata:
     name: ext-svc  //should be same
     namespace: dev
   subsets:
     - addresses:
       - ip: 125.209.222.142
       ports:
       - port: 80
---
apiVersion: v1
kind: Service
metadata:
    ...
spec:
  selector:
    [key]: [value]
  ports:
    - port: [svcPort]
      targetPort: [ctPort]
      
    Discovering Service
    - By ENV
        - when a pod is scheduled, kube inits a set of environment variables pointing to each service at that moment.
        - [SVC_NAME]_HOST & [SVC_NAME]_PORT
    - By DNS
        kube-dns is dns server pod in kube. (kube-system namespace)
        FQDN: [SVC_NAME].[NAMESPACE].svc.cluster.local
        
External Service (exposing service to external)
- NodePort
    - open a port on all nodes.
    - access by:
    [CLUSTER_IP]:[SVC_PORT]     - inner connection
    [ith Node's IP]:[NODE_PORT] - external connection
    - if the node you know fails, your clients can't access the service anymore. so should put load balancer
    
- LoadBalancer
    - makes the service accessible through a dedicated load balancer from cloud.
    - automatic provision of a load balancer from the cloud provider.
    
- Ingress
    - exposing multiple service through a single IP address. (operates at the HTTP level)
    - LoadBalancer requires its own LB and own pulbic IP (per service), whereas an Ingress only requires only one LB and public IP.
    multiple services can be exposed through as single ingress.
    - the host and path in url determine which service the req is forwarded. (By Controller)
    - ingress controller is required.
    
    ---
    apiVersion: v1
    kind: Service
    metadata:
      namespace: dev
      name: kubia-client
      labels:
        app: kubia-client
    spec:
      type: NodePort
      selector:
        app: kubia-client
      ports:
        - name: http
          port: 80
          targetPort: http
          nodePort: 30123
    ---
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      namespace: dev
      name: ingree
    spec:
      rules:
        - host: kubia.siwoo.com
          http:
            paths:
              - path: /
                backend:
                  serviceName: kubia-client
                  servicePort: 80
    
    
Volumes
- preserve pods' directories and hold actual data.
- share data between containers.
- a component of pod and defined in the pod's spec.
- mounted in each container that needs to access it.

Volume types.
    - emptyDir
    A simple empty directory used for storing transient data.
    The lifetime is tied to the pod.
    - hostPath
    Used for mounting directories from "node's filesystem"
    Pods running on the same node and using same hostPath volume see the same files.
    - nfs
    An NFS share mounted into the pod.
    - gcePersistentDisk, awsElasticBlockStore, azureDisk
    Used for mounting cloud provider-specific storage.
    configMap, secret, downwardAPI
    - special types of volumes used to expose kube's resources and cluster info.
    PVC (persistentVolumeClaim)
    - dynamically provisioned persistent storage.
  
emptyDir  
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
        ...
    spec:
      replicas: 3
      selector:
        matchExpressions:
            ...
      template:
        metadata:
            ...
        spec:
          containers:
            - name: ...
              image: ...
              volumeMounts:
              - name: [vlName]
                mountPath: [mntPath]
            - name: ...
              image: ...
              volumeMounts:
                - name: [vlName]
                  mountPath: [mntPath]
                  readOnly: true
              ports:
                ...
          volumes:
          - name: [vlName]
            emptyDir:
              medium: Memory

GKE Persistent Disk (Cloud Disk)
- a volume backed by the GCE persistent disk.
- rescheduled pod will see exact same disk, even if the pod is located in a different node.
  template:
    metadata:
      ...
    spec:
      volumes:
        - name: [vlName]
          gcePersistentDisk:
            pdName: [diskNameCreated]
            fsType: [filesystemType]
      containers:
        - name: ...
          image: ...
          volumeMounts:
            - mountPath: [mntPath]
              name: [vlName]
          ports:
            - containerPort: ...
              protocol: ...

PV & PVC (PersistentVolumes and PersistentVolumeClaims) 
- Decoupling pods from the underlying storage.
- PV are provisioned by cluster admins and consumed by pods through PVC.
- the developer doesn't need to know anything about the actual storage techonology.

PV
- admin sets up network storage then creates a PV by positing a PV descriptor with the storage info.
- PV doesn't belong to any namespace.
PVC
- user creates a PVC and kube finds a pv of adequate size and access mode and binds the PVC to the PV.
- then user creates a pod with a volume referencing the PVC
AccessModes
- RWO - ReadWriteOnce - Only "a single node" can mount the volume for reading and writing
- ROX - ReadOnlyMany - "Multiple nodes" can mount the volume for reading
- RWX - ReadWriteMany - "Multiple nodes" can mount the volume for reading and writing.

ReclaimPolicy
- after pvc delete, the status of pv will be Released (not Available). because it may contain data, so allow cluster admin a chance to clean it up.
- change the reclaim policy to Retain to prevent losing data.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: [pvName]
spec:
  capacity:
    storage: [pvSize]
  accessModes:
    - [allowedAccessMode]
    - [allowedAccessMode]
  persistentVolumeReclaimPolicy: [Retain|Delete|Recycle]
  [cloudDisk]:
    pdName: [diskName]
    fsType: [fsType]
    
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: [pvcName]
spec:
  resources:
    requests:
      storage: [pvcSize]
  accessModes:
    - [accessMode]
  storageClassName: ""
  
StorageClass (Dynamic provisioning of PV)
- without cluster admin, kue perform perform this job automatically through dynamic provisioning of pv.
- Instead of admin creating a bunch of pv, they define one or more StorageClass and let the system create a PV each time one is requested through PVC.
- "provisioner" should be specified for provisioning the PV.


---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: [stcName]
provisioner: [cloudProvisioner]
parameters:
    ...[cloudSpecificParameters]
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
  namespace: dev
spec:
  resources:
    requests:
      storage: [pvcSize]
  accessModes:
    - [accessMode]
  storageClassName: [stcName]