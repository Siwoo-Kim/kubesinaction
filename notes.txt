Kube Architecture
- Kube Control Plane
- Nodes

Kube Control Plane
- Kube Control Plane controls and makes the whole cluster function.

Kube Control Plane Components
- etcd (distributed persistent storage)
- Kube API Server.
- Scheduler
- Controller Manager.
- Kube DNS

Node Components.
- kubelet
- kube-proxy
- container runtime. (Docker)

Kube API server.
- all system components communicate through API server.
- provides a CRUD interface for querying and modifying the cluster state over a RESTful API
- send updated object to all interested watchers. (kube get pods --watch)

Controller Manager
- make sure the actual state of the system converges (수렴) toward the desired state.
- controllers run a loop, which reconciles the actual state with the desired state. 
    eg) Deployment controller keep the actual state of a deployment in sync with the desired state specified in object manifest. 
    eg) Persistent controller find an appropriate PV for PVC and bind it to the PVC.
    
Scheduler
- instructs which cluster node a pod should run on.
- as soon as the kubelet on the target node sees the pod has been scheduled to its node, it creates and runs the pod's containers.
- default scheduling algorithm - filtering all nodes and use RR (round-robin) to ensure pods are deployed across the nodes evenly.

etcd
- distributed key-value store to save object's manifest
- only Kube API server talks to etcd directly.

Kubelet
- Kubelet is the component responsible for everything running on a worker node.
- monitor the Kube API server for pods that have been scheduled to the node, and start the pod's container.
- telling Docker (container runtime) to run a container from a specific image.
- reports pod's status, events and resource consumption to the API server.
- runs liveness probes, restarting container when the probes fail.

Kube-proxy
- every node runs the kube-proxy
- make sure clients can connect to the services defined through "Kube API".
- When a service is baked by more than one pod (or endpoints), the proxy performs load balancing across the pods.
- userspace proxy mode & iptable proxy mode.

Kube DNS server.
- All the pods use the cluster's internal DNS server to look up services by name (Service discovery)
- The DNS service's IP is specified as the nameserver in the /etc/resolv.conf inside every container.

Ingress Controllers.
- runs a reverse proxy server and forward traffic to the service's pod directly.

Inter-pod networking
- NAT-less network. the packet sent by pod A must reach pod B with both the source & destination IP unchanged.
- Pod's network interface, a veth pair, Node's bridge.

Container
- Kubernetes requires app to be packaged into container image.
- Docker supports having multiple versions under same image.
- Docker file is instructions that Docker will perform when building the image.
- Images is composed of multiple layered.

    docker run busybox echo "Hello world"
    - run the image

GKE
- Use gcloud tool to configure default settings.
- https://cloud.google.com/kubernetes-engine/docs/quickstart

Run app on kubernetes
- prepare JSON or YAML manifest, containing a description of all the resources you want to deploy.

Components 
    Nodes
    - Each node runs "Docker", "Kubelet" and "kube-proxy".
    Master Node
    - controls while kube system (control plane).
    Kubectl
    - client command tool which issues REST requests to "Kube API Server"
    
    
Flow of deploying resources.
    1. developer create & push image to "Docker Hub"
    2. developer define & create resource using "kubectl"
    3. kubectl issues REST call to "Kube API Server"
    4. Resource is schedule to a worker node by "Scheduler"
    5. "Kubelet" on the node is notified.
    6. The Node pull the image and run the container from "Docker Hub"

Resource components
- "metadata" includes the name, labels and annotations about the resource.
- "spec" contains the actual description. (eg. contents, env, volumes or other config data)
- "status" contains the current info about the running resource.

Reference
- "kube explain [resourceType]" show you reference documentation for the given resource.
- https://kubernetes.io/docs/reference/kubernetes-api/

Namespace
- organize resources into groups.
- to isolate groups from each other.
    
apiVersion: v1
kind: Namespace
metadata:
  name: [nsName]

Pods
- basic building block in Kube.
- co-located group of containers that will runs on the same node.
- "logical machine" with its own IP, hostname, processes.
- pods spread out across different worker nodes.
- A pod never spans two nodes.

    Splitting into multiple pods.
    - Scaling for each demand.
    - Distribution for tacking advantage of the cluster's resource
    
    Deciding when to use multiple containers in a pod.
    - can they on different hosts?
    - Are then independent components?
    - Must test be scaled together or individually?
    
    Creating pods from yaml.
    - posting YAML manifest to the "Kube API Server"
    https://kubernetes.io/docs/reference/
    
    Container logs
    - kube logs [podName] -c [ctName]
    - automatically rotated daily and every time the log file reaches 10MB.
    
    Port Forwarding
    - no dedicated server, but can connecting to a pod for "debugging".
    - forwarding "local port" to "a port" in the pod.
    - "kubectl port-forward"
    
    Labels (metadata.labels)
        - Organizing Pods
        - key-value pair attached to a resource.
        - selecting resources using label selectors.
        - eg. "app" - kind of service, "rel" - release version
        - Label selectors allow you to select a subset of pods. "kube get -l" 
            - contains a label with a certain key
            - contains a label with a certain key and value
            - contains a label with a certain key, but with value not equal to the one you specify
        
        NodeSelector (spec.nodeSelector)
        - tells kube to deploy this pod only to nodes containing same label.
    
    Annotations (metadata.annotations)
    - hold much larger information than labels.
    
    apiVersion: v1
    kind: Pod
    metadata:
      name: [podName]
      labels:
        [key]: [value]
        ..
    spec:
      nodeselector:
        [key]: [value]
      containers:
        - name: [ctName]
          image: [imgName]
          env:
            - name: [envName]
              value: [envVal]
          ports:
            - name
              containerPort: [ctPort]
              protocol: TCP
              
    Liveness probes
    - check container's health through liveness probes.
    - specify "liveness probe" in the "pod's specification".
    - Kube will periodically execute and if a "probe fail", it will "restart" the container.
    - Liveness probes 은 헬스체킹한 후 문제가 있다면 팟을 재시작, Readness probes 은 헬스체킹 한 후 트래픽을 받지 않도록 설정.
    
        Components:
        HTTP GET probe
        - performs an HTTP GET request on the container's IP address, a port and path you specify.
        - "delay", "timeout", "period"
        - already remember to set an initial delay to account for app's startup time
        - should always define a liveness probe.
        
        TCP Socket probe
        - open a TCP connection to the specified port of the container. (check whether connection is established or not)
        
        Exec probe
        - executes an arbitrary command in the container.
        
    apiVersion: v1
    kind: Pod
    metadata:
        ...
    spec:
      containers:
        - name: kubia
        ...
          livenessProbe:
            failureThreshold: 1
            initialDelaySeconds: 15
            timeoutSeconds: 5
            periodSeconds: 15
            httpGet:
              port: 8080
              path: /
                  
    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=1
    
Job
- for a single completable task.
- in the event of a failure of the process, the Job can be configured to either restart the container or not.

apiVersion: batch/v1
kind: Job
metadata:
    ...
spec:
  template:
    metadata:
      labels:
        [key]: [value]
    spec:
      completions: [numOfTasks]
      parallelism: [numberOfThreads]
      restartPolicy: OnFailure
      containers:
        ...

ReplicationController
- used to replicate pods. (high-availability)

Service
- distribute the requests across multiple pods.
- a single constant IP and port for pods (pod is ephemeral and pod IP addresses are ever-changing)

ReplicationControllers
- a resource that ensures its pods are always kept running.
- create and manage multiple copies (replicas) of a pod.
- If pod disappears, rs notices the missing pod and creates a replacement pod. 
- makes sure the actual number of pods of a "a certain label" always matches the desired number (replicas)
- "pod template" can be modified at any time and will have effect on newly created pod.
- horizontally scale up by "kubectl scale rc [resourceName] --replicas=[replicas]"
- when you delete a rc through kubectl delete, the pods are also deleted. 
    Components:
    A Label Selector
    - determines what pods are in rc's scope
    A Replica Count
    - desired number of pods
    A Pod Template
    - is used when rc creates new pod replicas.

    Changing label for a pod.
    - if a pod is malfunctioning, take the pod out of the rc scope (by changing the label - rc will bring new one), 
    and then debug with the pod.
apiVersion: v1
kind: ReplicationController
metadata:
  ...
  labels:
    [key]: [value]
spec:
  replicas: [replicas]
  selector: #pod selector determining what pods the rc cares
    [key]: [value]
  template: #the pod template for creating new pods.
    metadata:
      ...
      labels:
        [key]: value
    spec:
      containers:
      ...

    ReplicaSet
    - new generation of rc.
    - has more expressive pod selectors.
    
    ReplicaSet Selector
        matchLabels
        - listing labels the pods need to have
        matchExpressions
        - match labels by expression "In", "NotIn", "Exists", "DoesNotExist"
        
    DemonSets
    - want a pod to run on each and every node in the cluster
    - useful for infrastructure-related pods that perform system-level operations.
    eg. log collector or resource monitor on every node or kube-proxy process.

Service
- respond to HTTP req coming either from pods inside the cluster of from clients outside the cluster.
- a single, constant point  of entry to a group of pods.
- each has an constant IP address and port.
- label selectors define which pods are part of the service.

    Problems
    - pods are ephemeral
    - IP is assigned after the pod has been scheduled to a node.
    - Pods are replicated & distributed.
   
   Components
   - CLUSTER-IP is for access from inside the cluster.
   - Session Affinity property allow client to be redirected to the same pod everytime.
   - port refers to "svc port"
   - targetPort refers to "pod's port"
   - nodePort refers to "node's port"
   - Discovering services via ENV or DNS
   
   EndPoints
   - EndPoints represents pods' ips and ports.
   - It's kube resource. [kube get endpoints [podName]]
   - Manually created endpoints can be used for external service. (the service can be used like any regual service)
   
   apiVersion: v1
   kind: Service
   metadata:
     name: ext-svc  //should be same
     namespace: dev
   spec:
     ports:
     - port: 80
   ---
   apiVersion: v1
   kind: Endpoints
   metadata:
     name: ext-svc  //should be same
     namespace: dev
   subsets:
     - addresses:
       - ip: 125.209.222.142
       ports:
       - port: 80
---
apiVersion: v1
kind: Service
metadata:
    ...
spec:
  selector:
    [key]: [value]
  ports:
    - port: [svcPort]
      targetPort: [ctPort]
      
    Discovering Service
    - By ENV
        - when a pod is scheduled, kube inits a set of environment variables pointing to each service at that moment.
        - [SVC_NAME]_HOST & [SVC_NAME]_PORT
    - By DNS
        kube-dns is dns server pod in kube. (kube-system namespace)
        FQDN: [SVC_NAME].[NAMESPACE].svc.cluster.local
        
External Service (exposing service to external)
- NodePort
    - open a port on all nodes.
    - access by:
    [CLUSTER_IP]:[SVC_PORT]     - inner connection
    [ith Node's IP]:[NODE_PORT] - external connection
    - if the node you know fails, your clients can't access the service anymore. so should put load balancer
    
- LoadBalancer
    - makes the service accessible through a dedicated load balancer from cloud.
    - automatic provision of a load balancer from the cloud provider.
    
- Ingress
    - exposing multiple service through a single IP address. (operates at the HTTP level)
    - LoadBalancer requires its own LB and own pulbic IP (per service), whereas an Ingress only requires only one LB and public IP.
    multiple services can be exposed through as single ingress.
    - the host and path in url determine which service the req is forwarded. (By Controller)
    - ingress controller is required.
    
    ---
    apiVersion: v1
    kind: Service
    metadata:
      namespace: dev
      name: kubia-client
      labels:
        app: kubia-client
    spec:
      type: NodePort
      selector:
        app: kubia-client
      ports:
        - name: http
          port: 80
          targetPort: http
          nodePort: 30123
    ---
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      namespace: dev
      name: ingree
    spec:
      rules:
        - host: kubia.siwoo.com
          http:
            paths:
              - path: /
                backend:
                  serviceName: kubia-client
                  servicePort: 80
    
    
Volumes
- preserve pods' directories and hold actual data.
- share data between containers.
- a component of pod and defined in the pod's spec.
- mounted in each container that needs to access it.

Volume types.
    - emptyDir
    A simple empty directory used for storing transient data.
    The lifetime is tied to the pod.
    - hostPath
    Used for mounting directories from "node's filesystem"
    Pods running on the same node and using same hostPath volume see the same files.
    - nfs
    An NFS share mounted into the pod.
    - gcePersistentDisk, awsElasticBlockStore, azureDisk
    Used for mounting cloud provider-specific storage.
    configMap, secret, downwardAPI
    - special types of volumes used to expose kube's resources and cluster info.
    PVC (persistentVolumeClaim)
    - dynamically provisioned persistent storage.
  
emptyDir  
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
        ...
    spec:
      replicas: 3
      selector:
        matchExpressions:
            ...
      template:
        metadata:
            ...
        spec:
          containers:
            - name: ...
              image: ...
              volumeMounts:
              - name: [vlName]
                mountPath: [mntPath]
            - name: ...
              image: ...
              volumeMounts:
                - name: [vlName]
                  mountPath: [mntPath]
                  readOnly: true
              ports:
                ...
          volumes:
          - name: [vlName]
            emptyDir:
              medium: Memory

GKE Persistent Disk (Cloud Disk)
- a volume backed by the GCE persistent disk.
- rescheduled pod will see exact same disk, even if the pod is located in a different node.
  template:
    metadata:
      ...
    spec:
      volumes:
        - name: [vlName]
          gcePersistentDisk:
            pdName: [diskNameCreated]
            fsType: [filesystemType]
      containers:
        - name: ...
          image: ...
          volumeMounts:
            - mountPath: [mntPath]
              name: [vlName]
          ports:
            - containerPort: ...
              protocol: ...

PV & PVC (PersistentVolumes and PersistentVolumeClaims) 
- Decoupling pods from the underlying storage.
- PV are provisioned by cluster admins and consumed by pods through PVC.
- the developer doesn't need to know anything about the actual storage techonology.

PV
- admin sets up network storage then creates a PV by positing a PV descriptor with the storage info.
- PV doesn't belong to any namespace.
PVC
- user creates a PVC and kube finds a pv of adequate size and access mode and binds the PVC to the PV.
- then user creates a pod with a volume referencing the PVC
AccessModes
- RWO - ReadWriteOnce - Only "a single node" can mount the volume for reading and writing
- ROX - ReadOnlyMany - "Multiple nodes" can mount the volume for reading
- RWX - ReadWriteMany - "Multiple nodes" can mount the volume for reading and writing.

ReclaimPolicy
- after pvc delete, the status of pv will be Released (not Available). because it may contain data, so allow cluster admin a chance to clean it up.
- change the reclaim policy to Retain to prevent losing data.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: [pvName]
spec:
  capacity:
    storage: [pvSize]
  accessModes:
    - [allowedAccessMode]
    - [allowedAccessMode]
  persistentVolumeReclaimPolicy: [Retain|Delete|Recycle]
  [cloudDisk]:
    pdName: [diskName]
    fsType: [fsType]
    
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: [pvcName]
spec:
  resources:
    requests:
      storage: [pvcSize]
  accessModes:
    - [accessMode]
  storageClassName: ""
  
StorageClass (Dynamic provisioning of PV)
- without cluster admin, kue perform perform this job automatically through dynamic provisioning of pv.
- Instead of admin creating a bunch of pv, they define one or more StorageClass and let the system create a PV each time one is requested through PVC.
- "provisioner" should be specified for provisioning the PV.


---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: [stcName]
provisioner: [cloudProvisioner]
parameters:
    ...[cloudSpecificParameters]
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
  namespace: dev
spec:
  resources:
    requests:
      storage: [pvcSize]
  accessModes:
    - [accessMode]
  storageClassName: [stcName]
  
Configuration Resource
- through command-line arguments
- through env variables.
- through mounting volumes (configmap, secret). This is preferred way. (decoupling configs from pod descriptor)

Passing command-line arguments to containers.
- Docker ENTRYPOINT: defines the executable invoked when the container is started.
- Docker CMD: specifies the args that passed to the ENTRYPOINT
- Kube command: override Docker ENTRYPOINT
- Kube args: override Docker CMD
- can refer env by $(ENV_NAME)

template:
metadata:
  labels:
    app: ...
spec:
  containers:
    - name: ...
      image: ...
      command:
        - [exec form]
      args:
        - [arg1]    // must enclose numbers
        - [arg2]
        ...
      volumeMounts:
        ...

Setting env variable for a container.
- kube allow you to specify env variables for each container.
- env variables cannot be updated after the pod is created.
- set the env variable "at the container level"
- can refer other env by $(ENV_NAME)

template:
    metadata:
      labels:
        ...: ...
      name: ...
    spec:
      containers:
        - name: ...
          image: ...
          env:
            - name: [name]
              value: [value]
            - name: [name1]
              value: [value]
            - name: [name2]
              value: $(name1)   //refer to other env
          volumeMounts:
            ...
      volumes:
        ...
        
ConfigMaps
- key/value pairs with the values ranging from literals to full config files.
- allows to keep same pod manifest manifests for different envs.
- only container refer the non-existing cm will fail to start, but other container in the same pod will start.
- can use kubectl create cm [cmName] --from-file=[file|directory] --from-literal=[key]=[value]
- you can mount the configmap as volume.

apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    ...
  name: [cmName]
data:
  [key]: [value]
  
  
template:
    metadata:
        ...
    spec:
      containers:
        ...
            env:
            - name: [envName]
              valueFrom:
                configMapKeyRef:
                  name: [cmName]
                  key: [keyName]
            - name: [envName]
              valueFrom:
                configMapKeyRef:
                  name: [cmName]
                  key: keyName
          volumeMounts:
            ...

Secrets
- hold key-value pairs that holds sensitive dta.
- pass secret entries to the container's env variables.
- expose secret entries as files in a volume.
- default-token attached to container automatically and that is used to talk with KUBE API server securely.
- default-token is in /var/run/secrets/kubernetes.io/serviceaccount
- secret can used for non-sensitive binary data. (1MB limit)

kube create secret generic fortune-http --from-file=[files]

  template:
    ...
    spec:
      containers:
        ...
          volumeMounts:
            ...
            - name: certs
              mountPath: [mntPath]
              readOnly: true
          ports:
            ...
      volumes:
        ...
        - name: certs
          secret:
            secretName: [secretName]
            
docker-registry Secret (Docker secrets)
- credentials for Docker registry.

kube create secret docker-registry [secretName] --docker-username=[userName] --docker-password=[pwd] --docker-email=[email]

Passing metadata through env or manifest. (Kube Downward API)
- The Downward API exposes pod metadata through env variables or files.
- must use downwardAPI volume for exposing the pod's labels or annotations.
- the metadata through downwardAPI volume is up-to-dated.

    Metadata from Downward API
    - pod's name
    - pod's ip
    - pod's namespace
    - node the pod is running
    - service account the pod is running
    - cpu and memory req & limit
    - pod's label & annotations
    
    Service account
    - the account that the pod authenticates when talking to the API server

template:
...
spec:
  containers:
      ...
      resources:
        requests:
          cpu: 15m
          memory: 100Ki
        limits:
          cpu: 100m
          memory: 4Mi
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
       - name: main-vol
                ...
            resources:
              requests:
                ...
            volumeMounts:
              - mountPath: /etc/downward
                name: downward-vol
    volumes:
      - name: downward-vol
        downwardAPI:
          items:
            - path: "name"
              fieldRef:
                fieldPath: metadata.name
                ...
            - path: "req_cpu"
              resourceFieldRef:
                containerName: main-vol
                resource: requests.cpu
                divisor: 1m
    
Kube API Server
- when need to know more about other pods' metadata, should talk with Kube API Server 
- Kube holds up-to-date information about other resources.
- "kube proxy" will run proxy server on local and allow to talk with "Kube API Server" (get its URL, by running kube cluster-info)
- from "Kube API Server" endpoints, can get the definition of any resources.
- in pod, ca.cert is in "/var/run/secrets/kubernetes.io/serviceaccount/" dir and can talk with "https://kubernetes"
- "Kube API Server"'s endpoint describe the REST resource exposed.
- https://10.8.0.1/api/v1/namespaces/[nsName]/[resourceType]/[resourceName]

Kube API Server Authentication.
- register ca.cert (/var/run/secrets/kubernetes.io/serviceaccount/ca.cert)
- -H "Authorization: Bearer: $TOKEN" (/var/run/secrets/kubernetes.io/serviceaccount/token)

Kube API Server Client Libraries to talk to the API server.
- fabric8io/kubernetes-client
- the library support https and take care of authentication.

Updating apps running in pods. (Deployment)
    Two ways of updating all the pods.
    - delete all existing pods first and then start the new ones. (=availability issue)
    - start new ones and, once they're up, delete the old ones. (replacing all or gradually) (=version conflict)
        = 모든 새로운 pod 을 생성후 교체하거나 하나의 팟 없애고 생성하는 식으로 점층적으로 교체.  
    Blue-Green Deployment
    - once all the new pods are up, change the Service's label selector and have the service switch over to the new pods.
    
Deployment
- higher-level resource for deploying application and updating them "declaratively", instead of rc and rs.
- A Deployment is backed by a replicaset internally.
- species a deployment strategy on the yaml.
- when create a deploy use [--record] option for revision history
- modifying pod template defined in deploy manifest will update the existing pods.
- roll out <> roll back
- must define readiness probe (don't send traffic to the removed pod.)

apiVersion: apps/v1
kind: Deployment
metadata:
    ...
spec:
  replicas: [numReplicas]
  strategy:
    rollingUpdate:
      maxSurge: 25%       // determine how man pod you allow to exist when update app.
      maxUnavailable: 25% // how many pod can be unavailable
    type: RollingUpdate
  selector:
    ...
  template:
    ...
    
    Deployment strategies.
    - RollingUpdate (default) - removes old pods ony by one, while adding new ones. (multi versions)
    - Recreate - delete all the old pods at once and then creates new ones. (short downtime)
    
Authentication
- client certificate.
- token in an HTTP header.
- basic HTTP authentication.

Users.
- two kinds of client: actual humans, pods (ServiceAccount).

Groups.
- human users and ServiceAccounts can belong to one or more groups.
- groups are used to grant permissions to several users at once.

    Built in groups.
    - system:unauthenticated
    - system:authenticated
    - system:serviceaccounts
    - system:serviceaccounts:<namespace>
    
ServiceAccount
- represents the identity of the app running in the pod.
- a way for an app in the pod to authenticate itself with the "Kube API server"
- "/var/run/secrets/kubernetes.io/serviceaccount/token" holds ServiceAccount's authentication token.
- usernames are formatted like: "system:serviceaccount:<namespace><service account name>"
- ServiceAccount is automatically created for each namespace.
- a pod can only use "a single ServiceAccount" "from the pod's namespace."
- specifying the account's name in the pod manifest and by doing this, you can control which resources each pod has access to (RBAC).
- When sa is created, associated ca.crt and token are automatically created. (only the pod has the serviceaccount can "mount" the token)
- ServiceAccount also contain a list of image pull Secrets.
- ServiceAccounts can assign by setting the name in the spec.serviceAccountName

---
apiVersion: v1
kind: ServiceAccount
metadata:
    ...
imagePullSecrets:
  - name: ...
---
apiVersion: v1
kind: Pod
metadata:
    ...
spec:
  serviceAccountName: foo
      containers:
        ...
      
RBAC (role-based access control)
- to check whether an action is allowed to be performed by the user.
- RBAC rule applying to whole resource types and specific instances of a resource.
- a user can have multiple RBAC rules.
- RBAC rules are configured through Roles - ClusterRoles or RoleBinding - ClusterRoleBinding.
- "A Role" only allow access to resources in the "same namespace".
- "A Cluster Role" allow access to resources across different namespaces.

    Actions
    - get pods (HTTP GET)
    - create services (HTTP POST)
    - update secrets (HTTP PUT)
    ..

    Verbs.
    HTTP method     verb for resource
    GET, HEAD       get (single), list(collection)
    POST            create
    PUT             update
    PATCH           patch
    DELETE          delete(single), deletecollection(collection)
    
    Roles (namespaced resources) & ClusterRoles (cluster-level resources)
    - specify which verbs can be performed on which resources.
    - define what can be done
    RoleBindings (namespaced resources) & ClusterRoleBindings (cluster-level resources)
    - bind the above roles to specific users, groups or ServiceAccount.
        binding "Subjects" with the "Role"
    - define who can do it.
    
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: [ns]
  name: [roleName]
rules:
  - apiGroups: [apiGroups]
    verbs: [verbs for actions]
    resources: [resourceTypes - plural]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: [ns]
  name: [rolebindingName]
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: [roleName]
subjects:
  - kind: [User|Group|ServiceAccount]
    name: [subjectName]
    namespace: [nsName]
    
Default Cluster Role
- view, edit, admin and cluster-admin

    "view" ClusterRole.
    - reading most resources in a namespace.
    "edit" ClusterRole.
    - allow to modify resources in a namespace.
    "admin" ClusterRole
    - grant full control.
    "cluster-admin"
    - grant full control & ResourceQuota objects